\begin{figure}[ht]
	\begin{algorithm}[H]
		\KwData{Ring size $n \in \N$, demands $d_{ij}$, $1 \leq i < j \leq n$.}
		Let $\phi$ be solution to the same instance in \RRL obtained using \cref{algo:rrl} and $S = \{(a_1, b_1), \ldots (a_m, b_m)\}$ the set of indices of split demands\;
		$D = \max_{1 \leq i < j \leq n} D_{ij}$\;
		$s = 0$\;
		\For{$i =1, \ldots, m$}{
			\If{$s+\phi(a_i, b_i) \leq \frac{D}{2}$}{
				$s = s + \phi(a_i, b_i)$\;
				$\phi(a_i, b_i) = 1$\;
			}
			\Else{
				$s = s - (1 - \phi(a_i, b_i))$\;
				$\phi(a_i, b_i) = 0$\;
			}
		}
%		Initialize $\theta_G$ and $\theta_D$\;
%		\While{not converged}{
%			\For{$i=1, \ldots, k$}{
%				Sample batch of $B$ samples $z_1, \ldots, z_B$ from $p_z$\;
%				Sample batch $x_{i_1}, \ldots, x_{i_B}$ from the training data\;
%				Compute the stochastic gradient 
%				$$\nabla_{\theta_D} \frac{1}{B} \sum_{j=1}^{B} \left( \ln D(x_{i_j}) + \ln(1 - D(G(z_j))) \right);$$
%				Update $\theta_D$ by ascending the gradient according to the learning rule $R$\;
%			}
%			Sample batch of $B$ samples $z_1, \ldots, z_B$ from $p_z$\;
%			Compute the stochastic gradient
%			$$\nabla_{\theta_G} \frac{1}{B} \sum_{j=1}^{B} -\ln(D(G(z_j)));$$
%			Update $\theta_G$ by descending the gradient according to the learning rule $R$\;
%		}
		\caption{Algorithm for \RL}
		\label{algo:rl}
	\end{algorithm}
\end{figure}