\begin{figure}[ht]
	\begin{algorithm}[H]
		\KwData{Ring size $n \in \N$, demands $d_{ij}$, $1 \leq i < j \leq n$.}
		$M = \max_{1 \leq i < j \leq n} D_{ij}$\;
		\For{$i=1, \ldots, n$}{
			$C_i = \max \left(\max_{1 \leq j < i}(D_{ji} - C_j), \max_{i < j \leq n}(D_{ij} - \frac{M}{2})\right)$ \;
		}
		\While{$\exists \text{parallel demands } d_{ij}, d_{gh}$}{
			Choose link $\{k, k+1\}$ that lies in between $d_{ij}$ and $d_{gh}$\;
			$\{k, l\} \coloneqq \min_{l \neq k} C_k + C_l - D_{kl}$\;
			Assume $d_{ij}$ crosses $\{k, l\}$\;
			Route $d_{gh}$ such that it misses $\{k, l\}$\;
			// Wollen cut constraint als Invariante idealerweise
		}
%		\While{not converged}{
%			\For{$i=1, \ldots, k$}{
%				Sample batch of $B$ samples $z_1, \ldots, z_B$ from $p_z$\;
%				Sample batch $x_{i_1}, \ldots, x_{i_B}$ from the training data\;
%				Compute the stochastic gradient 
%				$$\nabla_{\theta_D} \frac{1}{B} \sum_{j=1}^{B} \left( \ln D(x_{i_j}) + \ln(1 - D(G(z_j))) \right);$$
%				Update $\theta_D$ by ascending the gradient according to the learning rule $R$\;
%			}
%			Sample batch of $B$ samples $z_1, \ldots, z_B$ from $p_z$\;
%			Compute the stochastic gradient
%			$$\nabla_{\theta_G} \frac{1}{B} \sum_{j=1}^{B} -\ln(D(G(z_j)));$$
%			Update $\theta_G$ by descending the gradient according to the learning rule $R$\;
%		}
		\Return{$\phi$}
		\caption{Algorithm for \RRL}
		\label{algo:rrl}
	\end{algorithm}
\end{figure}